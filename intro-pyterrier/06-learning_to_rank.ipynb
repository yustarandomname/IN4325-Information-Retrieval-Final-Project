{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTerrier\n",
    "\n",
    "_IN4325: Information retrieval lecture, TU Delft_\n",
    "\n",
    "**Part 6: Learning to rank**\n",
    "\n",
    "In this part, we'll dive into learning-to-rank (LTR) models. Specifically, we'll cover how to use PyTerrier transformers to\n",
    "\n",
    "- compute query-document features and\n",
    "- train and evaluate LTR models.\n",
    "\n",
    "In order to run everything in this notebook, you'll need [NLTK](https://www.nltk.org/), [scikit-learn](https://scikit-learn.org/), and [LightGBM](https://github.com/microsoft/LightGBM/tree/master/python-package) installed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install python-terrier==0.10.0 nltk scikit-learn lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(tqdm=\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NLTK for tokenization later. This requires some data that we need to download first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `nfcorpus` dataset again, as before. In this notebook, we'll use a subset of the queries (`nontopic`). The only reason for this is that it makes the computations faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pt.get_dataset(\"irds:nfcorpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LTR models rely on query and document features, we'll include some metadata in our index, namely the titles, abstracts, and URLs.\n",
    "\n",
    "Note that this seems to slow down retrieval quite a bit (even when we're not retrieving the metadata from the index), so this notebook might run slower on your machine than the previous ones. We'll use caching on most transformers to mitigate this problem at least somewhat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "index = pt.index.IterDictIndexer(\n",
    "    str(Path.cwd()),  # this will be ignored\n",
    "    meta={\n",
    "        \"docno\": 16,\n",
    "        \"title\": 256,\n",
    "        \"abstract\": 65536,\n",
    "        \"url\": 128,\n",
    "    },\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ").index(dataset.get_corpus_iter(), fields=[\"title\", \"abstract\", \"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTR paradigm\n",
    "\n",
    "The idea of learning-to-rank is to use a feature-based supervised machine learning model for ranking. PyTerrier supports end-to-end LTR pipelines, including first-stage retrieval, computation of features, training, and evaluation.\n",
    "\n",
    "### First-stage retrieval\n",
    "\n",
    "LTR models are commonly used in a two-stage process (_retrieve-and-re-rank_): A lightweight retrieval model is used for _candidate selection_ given a query, and the LTR model subsequently _re-ranks_ the candidates. This is because applying the LTR model directly on the whole corpus would be too expensive.\n",
    "\n",
    "We'll use good old BM25 for first-stage retrieval. In order to keep the runtime of this notebook down, we limit the number of documents to be re-ranked to `100`. We also include the metadata of the retrieved documents so we can use it to compute features later:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage_bm25 = pt.BatchRetrieve(\n",
    "    index,\n",
    "    wmodel=\"BM25\",\n",
    "    num_results=100,\n",
    "    metadata=[\"docno\", \"title\", \"abstract\", \"url\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing features\n",
    "\n",
    "In order to compute features, we can use PyTerrier transformers. Specifically, the `**` operator (_feature union_) collects features computed by transformers in a designated column in the data frame.\n",
    "\n",
    "In order to illustrate this, we can use other retrievers to compute features. By applying the feature operator, we instruct PyTerrier to use these models for _scoring_ rather than retrieval. Here, we initialize two `pyterrier.BatchRetrieve` objects with the PL2 and DPH weighting models and include them in the pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features = ~first_stage_bm25 >> (\n",
    "    pt.BatchRetrieve(index, wmodel=\"PL2\") ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this pipeline does is the following: For each query,\n",
    "\n",
    "1. retrieve the top-`100` documents using the first-stage retriever (BM25), and\n",
    "2. compute the PL2 and DPH scores for each query-document pair (these are the features).\n",
    "\n",
    "Let's run this pipeline on a single query from the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics()\n",
    "pipeline_with_features(test_queries[test_queries[\"qid\"] == \"PLAIN-102\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to one of the candidate documents for this query. The `score` column contains the first-stage retrieval score (BM25), by which the documents are ordered. Finally, the `features` column contains the list of features. In our case, the first feature is the PL2 score, and the second feature is the DPH score.\n",
    "\n",
    "#### Custom features\n",
    "\n",
    "It is also easy to compute our own features. This can be done with custom transformers.\n",
    "\n",
    "Say, for example, we want to compute very simple similarity scores of the query to the title and abstract of each document, respectively. We can do this by implementing a function that takes as input a single row of the data frame (as above) and outputs a list of features (as a `numpy.ndarray`). In our case, we compute the Jaccard similarity of the query to the title and abstract:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _jaccard_sim(row):\n",
    "    query_tokens = set(nltk.word_tokenize(row[\"query\"].lower()))\n",
    "    title_tokens = set(nltk.word_tokenize(row[\"title\"].lower()))\n",
    "    abstract_tokens = set(nltk.word_tokenize(row[\"abstract\"].lower()))\n",
    "    js_query_title = len(query_tokens & title_tokens) / len(query_tokens | title_tokens)\n",
    "    js_query_abstract = len(query_tokens & abstract_tokens) / len(\n",
    "        query_tokens | abstract_tokens\n",
    "    )\n",
    "    return np.array([js_query_title, js_query_abstract])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Side note: This way of doing it is inefficient, because each query is tokenized multiple times. Can you think of a better way of implementing this?_\n",
    "\n",
    "We can now include this function as a transformer in our pipeline by using [`pyterrier.apply.doc_features`](https://pyterrier.readthedocs.io/en/latest/apply.html#pyterrier.apply.doc_features).\n",
    "\n",
    "You might have noticed that we're accessing the `title` and `abstract` columns in the data frame. This is possible because we added them as metadata during indexing and specified the metadata to be retrieved by `first_stage_bm25`. Alternatively, you can use the [`pyterrier.text.get_text`](https://pyterrier.readthedocs.io/en/latest/text.html#pyterrier.text.get_text) transformer to retrieve metadata from the index.\n",
    "\n",
    "Our new pipeline looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features = ~first_stage_bm25 >> (\n",
    "    pt.apply.doc_features(_jaccard_sim)\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same query through the new pipeline, we can see that our four features show up in the list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_features(test_queries[test_queries[\"qid\"] == \"PLAIN-102\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to compute a single feature in your custom transformer, you can use [`pyterrier.apply.doc_score`](https://pyterrier.readthedocs.io/en/latest/apply.html#pyterrier.apply.doc_score). Let's add two more features:\n",
    "\n",
    "1. By returning `row[\"score\"]`, we're simply adding the first-stage retrieval score to the feature set.\n",
    "2. We'll also include the length of the URL as a feature.\n",
    "\n",
    "We now have a complete pipeline with six features in total:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_complete = ~first_stage_bm25 >> (\n",
    "    pt.apply.doc_features(_jaccard_sim)\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "    ** pt.BatchRetrieve(index, wmodel=\"DPH\")\n",
    "    ** pt.apply.doc_score(lambda row: row[\"score\"])\n",
    "    ** pt.apply.doc_score(lambda row: len(row[\"url\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LTR models\n",
    "\n",
    "The actual models used for re-ranking are not implemented in PyTerrier itself; rather, PyTerrier provides a transformer for trainable models (i.e., regression or LTR models) that implement a scikit-learn-like API (i.e., `fit` and `predict` methods). These trainable transformers are [`pyterrier.Estimator`](https://pyterrier.readthedocs.io/en/latest/transformer.html#pt-transformer-estimator) objects.\n",
    "\n",
    "We'll start by training a simple SVM regression model from scikit-learn. Estimators can be created using [`pyterrier.ltr.apply_learned_model`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.apply_learned_model):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "ltr_svm = ~pipeline_complete >> pt.ltr.apply_learned_model(SVR())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do re-ranking, the model needs to be trained. The `nfcorpus` dataset provides a train/dev/test split, so we can easily load the training data.\n",
    "\n",
    "**Depending on your hardware, the next cell might take a while to execute.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_svm.fit(\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use gradient boosting methods from [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/) by specifying `form=\"ltr\"`. Let's train a [`lightgbm.LGBMRanker`](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html), which defaults to a LambdaMART model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRanker\n",
    "\n",
    "ltr_lambdamart = ~pipeline_complete >> pt.ltr.apply_learned_model(\n",
    "    LGBMRanker(\n",
    "        metric=\"ndcg\",\n",
    "        importance_type=\"gain\",\n",
    "    ),\n",
    "    form=\"ltr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model makes use of validation (dev) data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_lambdamart.fit(\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_qrels(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the baseline performance (no LTR) with the SVM and LambdaMART models on the test set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.measures import nDCG, RR, MAP\n",
    "\n",
    "pt.Experiment(\n",
    "    [first_stage_bm25, ltr_svm, ltr_lambdamart],\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_qrels(),\n",
    "    names=[\"BM25\", \"BM25 >> LTR (SVM)\", \"BM25 >> LTR (LambdaMART)\"],\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeaturesBatchRetrieve\n",
    "\n",
    "So far, we have used the feature union operator (`**`) to append PL2 and DPH scores to our feature list. This is not optimal, because each of the operations requires another index access to compute the features. If we're only interested in those retrieval-based features, we can use [`pyterrier.FeaturesBatchRetrieve`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.FeaturesBatchRetrieve) instead, which computes everything at once:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_fbr = pt.FeaturesBatchRetrieve(\n",
    "    index,\n",
    "    wmodel=\"BM25\",\n",
    "    features=[\"WMODEL:BM25\", \"WMODEL:PL2\", \"WMODEL:DPH\"],\n",
    "    num_results=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature ablation\n",
    "\n",
    "Given our approach above (`bm25_fbr`) with three features, we might be interested to know which of these features has the greatest impact on ranking performance. In order to find out, we could create three separate pipelines, where each of them has one of the features removed, and then compare the performance.\n",
    "\n",
    "Luckily, PyTerrier has us covered and provides transformers to make our lives easier: [`pyterrier.ltr.ablate_features`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.ablate_features) can be included in a pipeline to dynamically remove a set of features; [`pyterrier.ltr.keep_features`](https://pyterrier.readthedocs.io/en/latest/ltr.html#pyterrier.ltr.keep_features) does the opposite. Hence, we can simply use it in a loop to get the effect we want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_lambdamart_abl = {\n",
    "    feature: bm25_fbr\n",
    "    >> pt.ltr.ablate_features(feature)\n",
    "    >> pt.ltr.apply_learned_model(\n",
    "        LGBMRanker(\n",
    "            metric=\"ndcg\",\n",
    "            importance_type=\"gain\",\n",
    "        ),\n",
    "        form=\"ltr\",\n",
    "    )\n",
    "    for feature in [0, 1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to train each of these models individually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipeline in ltr_lambdamart_abl.values():\n",
    "    pipeline.fit(\n",
    "        pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_topics(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/train/nontopic\").get_qrels(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_topics(),\n",
    "        pt.get_dataset(\"irds:nfcorpus/dev/nontopic\").get_qrels(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines, names = [], []\n",
    "for feature, pipeline in ltr_lambdamart_abl.items():\n",
    "    pipelines.append(pipeline)\n",
    "    names.append(f\"LambdaMART (feature {feature} removed)\")\n",
    "\n",
    "pt.Experiment(\n",
    "    pipelines,\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_topics(),\n",
    "    pt.get_dataset(\"irds:nfcorpus/test/nontopic\").get_qrels(),\n",
    "    names=names,\n",
    "    eval_metrics=[nDCG @ 10, RR @ 10, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "Check out the [LTR section](https://pyterrier.readthedocs.io/en/latest/ltr.html) in the PyTerrier documentation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
