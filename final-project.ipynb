{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Project\n",
        "\n",
        "Libraries used:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install numpy deep-translator python-dotenv python-terrier==0.10.0 unidecode sent2vec scipy sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Initialising libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import dotenv_values\n",
        "%load_ext dotenv\n",
        "%dotenv\n",
        "\n",
        "env = dotenv_values(\".env\")  # replace \".env.example\" with .env file path\n",
        "\n",
        "if (env[\"LANGUAGE_DETECT_API_KEY\"] == \"YOUR API KEY\"):\n",
        "  raise Exception(\"Please replace 'YOUR API KEY' with your actual API key in the .env file\")\n",
        "\n",
        "detection_api_key = env[\"LANGUAGE_DETECT_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import threading\n",
        "\n",
        "import deep_translator as dt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyterrier as pt\n",
        "from sent2vec.vectorizer import Vectorizer\n",
        "from scipy import spatial\n",
        "from deep_translator import GoogleTranslator, single_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"numpy version:\", np.__version__)\n",
        "print(\"deep-translator version:\", dt.__version__)\n",
        "print(\"pyterrier version:\", pt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not pt.started():\n",
        "    pt.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Query formulation\n",
        "\n",
        "Let the end-user determine what they would like to find\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q0 = \"How do I repair my bike?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Query language identification\n",
        "\n",
        "Identify the language of the query with the help of the ... library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "l0 = single_detection(q0, api_key=detection_api_key)\n",
        "print(l0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Query translation\n",
        "\n",
        "Translate the query into Dutch, English, French, German, Italian, Portuguese, Russian, Spanish, and Chineese. Exlude the original language of the query from the translation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qs = dict({l0: q0})\n",
        "\n",
        "languages = [\"en\", \"fr\", \"it\", \"es\"]\n",
        "\n",
        "threads = []\n",
        "\n",
        "def translate(lang):\n",
        "    gt = GoogleTranslator(source=l0, target=lang)\n",
        "    translated = gt.translate(text=q0)\n",
        "    qs[lang] = translated\n",
        "\n",
        "\n",
        "for lang in languages:\n",
        "    if lang != l0:\n",
        "        t1 = threading.Thread(target=translate, args=(lang,))\n",
        "        t1.start()\n",
        "        threads.append(t1)\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "qs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Search for documents in the target language\n",
        "\n",
        "Search for documents in the target language using the translated queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References:\n",
        "1. MLWIKIR: APython toolkit for building large-scale Wikipedia-based Information Retrieval Datasets in Chinese, English, French, Italian, Japanese, Spanish and more. [Research paper](https://www.irit.fr/CIRCLE/wp-content/uploads/2020/06/CIRCLE20_22.pdf)\n",
        "1. [pyterrier jupyter notebook example of spanish document retreival](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/non_en_retrieval.ipynb)\n",
        "1. [WikIR rawa datasets](https://ir-datasets.com/wikir.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Indexing of the documents\n",
        "If the index of the documents is not available, then the documents will be indexed using the pyterrier library.\n",
        "Otherwise, the index will be loaded from the disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_index(dataset: str, index_name: str, fields=[\"text\"]): \n",
        "    \"\"\"\n",
        "    Creates an index for a given dataset using the specified index name and fields.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset object containing the corpus to be indexed.\n",
        "    - index_name: The name of the index to be created.\n",
        "    - fields: A list of fields to be indexed. Default is [\"text\"].\n",
        "\n",
        "    Returns:\n",
        "    - index_ref: The reference to the created index.\n",
        "\n",
        "    \"\"\"\n",
        "    indexer = pt.IterDictIndexer(\"./indices/\" + index_name, verbose=False)\n",
        "    index_ref = indexer.index(dataset.get_corpus_iter(), fields=fields)\n",
        "    return index_ref\n",
        "\n",
        "\n",
        "def find_index(index_name: str):\n",
        "    \"\"\"\n",
        "    Finds the reference to an existing index with the specified name.\n",
        "\n",
        "    Parameters:\n",
        "    - index_name: The name of the index to be found.\n",
        "\n",
        "    Returns:\n",
        "    - index_ref: The reference to the found index.\n",
        "\n",
        "    \"\"\"\n",
        "    return pt.IndexRef.of(\"./indices/\" + index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasetNames: dict[str, str] = dict(\n",
        "    {\"fr\": \"wikir/fr14k\", \"es\": \"wikir/es13k\", \"en\": \"wikir/en1k\", \"it\": \"wikir/it16k\"}\n",
        ")\n",
        "datasets = dict()\n",
        "indeces = dict()\n",
        "\n",
        "def index(dataset: str, index_name: str, fields=[\"text\"]):\n",
        "    index_ref = create_index(dataset, index_name, fields)\n",
        "    indeces[lang] = index_ref\n",
        "\n",
        "index_threads = []\n",
        "\n",
        "for [lang, datasetName] in datasetNames.items():\n",
        "    datasetFolder = datasetName.replace(\"/\", \"_\")\n",
        "    dataset = pt.get_dataset(\"irds:\"+datasetName)\n",
        "    datasets[lang] = dataset\n",
        "\n",
        "    if os.path.exists(\"./indices/\" + datasetFolder + \"/data.properties\"):\n",
        "        print(\"Index\", datasetFolder, \"already exists\")\n",
        "        index_ref = find_index(datasetFolder)\n",
        "        indeces[lang] = index_ref\n",
        "    else:\n",
        "        print(\"Creating index\", datasetFolder, \" (takes around 1-3 minutes per dataset)\")\n",
        "        thread = threading.Thread(target=index, args=(dataset, datasetFolder))\n",
        "        thread.start()\n",
        "        index_threads.append(thread)\n",
        "\n",
        "for thread in index_threads:\n",
        "    thread.join()\n",
        "\n",
        "print(indeces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Retrieval of the documents\n",
        "The documents will be retrieved using the BM25 retrieval model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import unidecode\n",
        "\n",
        "def sanitise_query(query: str):\n",
        "    \"\"\"\n",
        "    Sanitises a query by removing special characters and converting it to lowercase.\n",
        "\n",
        "    Parameters:\n",
        "    - query: The query to be sanitised.\n",
        "\n",
        "    Returns:\n",
        "    - sanitised_query: The sanitised query.\n",
        "\n",
        "    \"\"\"\n",
        "    decoded_query = unidecode.unidecode(query)\n",
        "    sanitised_query = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", decoded_query)\n",
        "    return sanitised_query.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document_data: dict[str, pd.DataFrame] = dict()\n",
        "NUM_RESULTS = 20\n",
        "\n",
        "for lang in languages:\n",
        "    index_ref = indeces[lang]\n",
        "    dataset = datasets[lang]\n",
        "\n",
        "    pipeline = pt.BatchRetrieve(\n",
        "        index_ref, wmodel=\"BM25\", metadata=[\"docno\"], num_results=NUM_RESULTS\n",
        "    ) >> pt.text.get_text(dataset, \"text\")\n",
        "\n",
        "    sanitised_query = sanitise_query(qs[lang])\n",
        "\n",
        "    pandas_df: pd.DataFrame = pipeline.search(sanitised_query)\n",
        "    document_data[lang] = pandas_df\n",
        "\n",
        "print(\n",
        "    \"Results for fr\",\n",
        "    document_data[\"fr\"].keys(),\n",
        "    \"- shape:\",\n",
        "    document_data[\"fr\"].shape,\n",
        "    \"top 5:\"\n",
        ")\n",
        "\n",
        "document_data[\"fr\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Document translation\n",
        "\n",
        "Translate the documents back to English to be processed by other algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents_translated_dict: dict[str, list[str]] = {}\n",
        "\n",
        "def translate_document(lang: str, index: int, document_text: str):\n",
        "    gt = GoogleTranslator(source=lang, target=\"en\")\n",
        "    translated = gt.translate(text=document_text)\n",
        "\n",
        "    documents_translated_dict[lang][index] = translated\n",
        "\n",
        "\n",
        "threads = []\n",
        "\n",
        "for lang, docs in document_data.items():\n",
        "    text_docs: list[str] = docs[\"text\"].tolist()\n",
        "    documents_translated_dict[lang] = [None] * len(text_docs)\n",
        "\n",
        "    for index, text in enumerate(text_docs):\n",
        "        t = threading.Thread(target=translate_document, args=(lang, index, text))\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "# Join the translated documents to one list\n",
        "documents_translated = []\n",
        "for lang, docs in documents_translated_dict.items():\n",
        "    documents_translated += docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Find domain-specific keywords\n",
        "\n",
        "Find the most frequent words in the documents, exlude the 1000 most used words in the English language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# We have N documents per language and L langauges\n",
        "\n",
        "# Convert each document to a set of words associated with its occurrence\n",
        "def document_to_wordset(doc):\n",
        "    # Create list of all words in doc\n",
        "    allwords_list = doc.split(\" \")\n",
        "\n",
        "    # Length of this list is the total number of terms in doc\n",
        "    total_terms = len(allwords_list)\n",
        "\n",
        "    # Create a dictionary with terms as keys and occurrence count\n",
        "    occurrence_set = Counter(allwords_list)\n",
        "\n",
        "    return occurrence_set, total_terms\n",
        "\n",
        "def compute_term_frequencies(doc_wordset: Counter, total_terms: int) -> Counter:\n",
        "    result = doc_wordset.copy()\n",
        "    # Simply divide the word occurrence in the wordset by the total number of terms in the document\n",
        "    for term in result.keys():\n",
        "        result[term] /= total_terms\n",
        "\n",
        "    return result\n",
        "\n",
        "def compute_idf(total_docs_in_corpus, docs_contain_term):\n",
        "    return math.log((1 + total_docs_in_corpus)/(1 + docs_contain_term))\n",
        "\n",
        "def compute_tfidf_per_term(frequency_wordsets: list[Counter]) -> Counter:\n",
        "    # WARN: there are terms which do not occur in every document\n",
        "\n",
        "    # Start by creating a counter that contains each term and the\n",
        "    # number of documents that term appeared in\n",
        "    document_occurrence_counter = Counter()\n",
        "\n",
        "    for frequency_wordset in frequency_wordsets:\n",
        "        for term in frequency_wordset.keys():\n",
        "            # If first encounter; set to one\n",
        "            if (term not in document_occurrence_counter.keys()):\n",
        "                document_occurrence_counter[term] = 1\n",
        "            # Otherwise add one encounter\n",
        "            else:\n",
        "                document_occurrence_counter[term] += 1\n",
        "\n",
        "    # For each term, compute the average term frequency,\n",
        "    # using the document count in aggregate_counter\n",
        "    avg_term_frequency_counter = Counter()\n",
        "\n",
        "    for term in document_occurrence_counter.keys():\n",
        "\n",
        "        avg_term_frequency_counter[term] = 0\n",
        "\n",
        "        for frequency_wordset in frequency_wordsets:\n",
        "            if (term in frequency_wordset.keys()):\n",
        "                avg_term_frequency_counter[term] += frequency_wordset[term]\n",
        "\n",
        "        avg_term_frequency_counter[term] /= document_occurrence_counter[term]\n",
        "\n",
        "    # For each term, compute the inverse document frequency for the documents\n",
        "    # in the given list\n",
        "    term_inverse_document_frequency_counter = Counter()\n",
        "\n",
        "    # The total number of documents in the corpus is simply in this case\n",
        "    total_documents = len(frequency_wordsets)\n",
        "\n",
        "    # For each term\n",
        "    for term in document_occurrence_counter.keys():\n",
        "        term_inverse_document_frequency_counter[term] = compute_idf(total_documents, document_occurrence_counter[term])\n",
        "\n",
        "    \n",
        "    # Finally, combine tf-idf for each term using the above two created counters\n",
        "    tfidf_counter = Counter()\n",
        "\n",
        "    for term in document_occurrence_counter.keys():\n",
        "        tfidf_counter[term] = avg_term_frequency_counter[term] * term_inverse_document_frequency_counter[term]\n",
        "\n",
        "    return tfidf_counter\n",
        "\n",
        "\n",
        "all_frequency_wordsets: list[Counter] = []\n",
        "\n",
        "for doc in documents_translated:\n",
        "    doc_wordset, total_terms = document_to_wordset(doc)\n",
        "\n",
        "    frequency_wordset = compute_term_frequencies(doc_wordset, total_terms)\n",
        "    all_frequency_wordsets.append(frequency_wordset)\n",
        "\n",
        "tfidf_per_term = compute_tfidf_per_term(all_frequency_wordsets)\n",
        "\n",
        "# Based on tfidf, extract the 5 'most domain-specific' terms\n",
        "domain_specific_terms = [item[0] for item in tfidf_per_term.most_common(100)]\n",
        "\n",
        "print(domain_specific_terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Rank to the domain specific keywords with word net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = Vectorizer()\n",
        "vectorizer.run(documents_translated)\n",
        "vectors = vectorizer.vectors\n",
        "\n",
        "q0_sanitised = sanitise_query(q0)\n",
        "q0_vectorizer = Vectorizer()\n",
        "q0_vectorizer.run([q0_sanitised])\n",
        "\n",
        "data = {\"distance\": [], \"document\": []}\n",
        "\n",
        "for i in range(0, len(documents_translated)):\n",
        "    similarity = spatial.distance.cosine(q0_vectorizer.vectors[0], vectors[i])\n",
        "    data[\"distance\"].append(similarity)\n",
        "    data[\"document\"].append(documents_translated[i])\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.sort_values(by=[\"distance\"], ascending=True, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"best doc:\", df[\"document\"][56])\n",
        "print(\"worst doc:\", df[\"document\"][60])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = Vectorizer()\n",
        "terms = [\"bike\", \"bikes\", \"citybike\", \"citybikes\", \"Citybikes\", \"sushi\", \"dog\"]\n",
        "vectorizer.run(terms)\n",
        "vectors = vectorizer.vectors\n",
        "\n",
        "for i in range(1, len(vectors)):\n",
        "  term = terms[i]\n",
        "  vector = vectors[i]\n",
        "  dist = spatial.distance.cosine(vectors[0], vector)\n",
        "  print(\"Distance between 'bike' and\", term, \":\", dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "q0s_sanitised = sanitise_query(q0).split(\" \")\n",
        "\n",
        "q0_vectorizer = Vectorizer()\n",
        "q0_vectorizer.run(q0s_sanitised)\n",
        "\n",
        "dst_q0_vectorizer = Vectorizer()\n",
        "dst_q0_vectorizer.run(domain_specific_terms)\n",
        "\n",
        "data = dict({\"q0 term\": [], \"dst\": [], \"distance\": []})\n",
        "\n",
        "for q0_i in range(0, len(q0_vectorizer.vectors)):\n",
        "    for dst_i in range(0, len(dst_q0_vectorizer.vectors)):\n",
        "        q0_vec = q0_vectorizer.vectors[q0_i]\n",
        "        dst_vec = dst_q0_vectorizer.vectors[dst_i]\n",
        "\n",
        "        distance = spatial.distance.cosine(q0_vec, dst_vec)\n",
        "        data[\"q0 term\"].append(q0s_sanitised[q0_i])\n",
        "        data[\"dst\"].append(domain_specific_terms[dst_i])\n",
        "        data[\"distance\"].append(distance)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.sort_values(by=[\"distance\"], ascending=True, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "vectorizer.run([q0] + [sanitise_query(x) for x in documents_translated])\n",
        "vectors = vectorizer.vectors\n",
        "\n",
        "data = dict({\"q0\": [], \"doc\": [], \"distance\": []})\n",
        "\n",
        "\n",
        "for i in range(1, len(vectors)):\n",
        "    dist = spatial.distance.cosine(vectors[0], vectors[i])\n",
        "    doc = documents_translated[i - 1]\n",
        "    data[\"q0\"].append(q0)\n",
        "    data[\"doc\"].append(doc)\n",
        "    data[\"distance\"].append(dist)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.sort_values(by=[\"distance\"], ascending=True, inplace=True)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Concat the keywords with the original query\n",
        "\n",
        "Concatenate the keywords with the original query and search for documents in the original language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In (5), we've extracted a number of domain specific terms that we found in the documents\n",
        "# in various different languages translated to the language of the original query.\n",
        "# Here, we will concatenate these domain specific terms to the original query to obtain\n",
        "# our reformulated query.\n",
        "\n",
        "reformulated_query = q0\n",
        "\n",
        "for term in domain_specific_terms:\n",
        "    reformulated_query += \" \" + term\n",
        "\n",
        "print(reformulated_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluation\n",
        "\n",
        "Evaluate the results of the search\n",
        "\n",
        "Reference:\n",
        "1. GitHub: [pyterrier/examples/notebooks\n",
        "/retrieval_and_evaluation.ipynb](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/retrieval_and_evaluation.ipynb)\n",
        "1. GitHub: [pyterrier/examples/notebooks\n",
        "/experiment.ipynb](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/experiment.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Precision / Recall\n",
        "\n",
        "See how many of the returned documents are relevant. Did the number of relevant documents increase?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Keyword diversification\n",
        "\n",
        "Did the number of unique keywords increase compared to naive domain-specific keyword identification?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
