{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "Libraries used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: deep-translator in ./.venv/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: python-terrier==0.10.0 in ./.venv/lib/python3.10/site-packages (0.10.0)\n",
      "Collecting ray\n",
      "  Downloading ray-2.10.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (2.2.1)\n",
      "Requirement already satisfied: wget in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (3.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (4.66.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.6.1)\n",
      "Requirement already satisfied: matchpy in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.5)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.1.post1)\n",
      "Requirement already satisfied: deprecated in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.2.14)\n",
      "Requirement already satisfied: chest in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.2.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.12.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (2.31.0)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.3.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (1.4.4)\n",
      "Requirement already satisfied: more-itertools in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (10.2.0)\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (3.1.3)\n",
      "Requirement already satisfied: statsmodels in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.14.1)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.3)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.3.8)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in ./.venv/lib/python3.10/site-packages (from python-terrier==0.10.0) (0.5.6)\n",
      "Requirement already satisfied: typish>=1.7.0 in ./.venv/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier==0.10.0) (1.9.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in ./.venv/lib/python3.10/site-packages (from deep-translator) (4.12.3)\n",
      "Collecting click>=7.0 (from ray)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting filelock (from ray)\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting jsonschema (from ray)\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray)\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from ray) (24.0)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3 (from ray)\n",
      "  Downloading protobuf-5.26.0-cp37-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.10/site-packages (from ray) (6.0.1)\n",
      "Collecting aiosignal (from ray)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting frozenlist (from ray)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (5.1.0)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.6)\n",
      "Requirement already satisfied: ijson>=3.1.3 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (3.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.1.12)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in ./.venv/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier==0.10.0) (0.2.2)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in ./.venv/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier==0.10.0) (1.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->python-terrier==0.10.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->python-terrier==0.10.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->python-terrier==0.10.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->python-terrier==0.10.0) (2024.2.2)\n",
      "Requirement already satisfied: heapdict in ./.venv/lib/python3.10/site-packages (from chest->python-terrier==0.10.0) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./.venv/lib/python3.10/site-packages (from deprecated->python-terrier==0.10.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->python-terrier==0.10.0) (2.1.5)\n",
      "Collecting attrs>=22.2.0 (from jsonschema->ray)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->ray)\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->ray)\n",
      "  Downloading referencing-0.34.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->ray)\n",
      "  Downloading rpds_py-0.18.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from matchpy->python-terrier==0.10.0) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->python-terrier==0.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->python-terrier==0.10.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->python-terrier==0.10.0) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->python-terrier==0.10.0) (3.4.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in ./.venv/lib/python3.10/site-packages (from statsmodels->python-terrier==0.10.0) (0.5.6)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from patsy>=0.5.4->statsmodels->python-terrier==0.10.0) (1.16.0)\n",
      "Requirement already satisfied: cbor>=1.0.0 in ./.venv/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier==0.10.0) (1.0.0)\n",
      "Downloading ray-2.10.0-cp310-cp310-macosx_11_0_arm64.whl (63.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading msgpack-1.0.8-cp310-cp310-macosx_11_0_arm64.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.0-cp37-abi3-macosx_10_9_universal2.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.0/404.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Using cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Downloading referencing-0.34.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.18.0-cp310-cp310-macosx_11_0_arm64.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.7/330.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rpds-py, protobuf, msgpack, frozenlist, filelock, click, attrs, referencing, aiosignal, jsonschema-specifications, jsonschema, ray\n",
      "Successfully installed aiosignal-1.3.1 attrs-23.2.0 click-8.1.7 filelock-3.13.3 frozenlist-1.4.1 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 msgpack-1.0.8 protobuf-5.26.0 ray-2.10.0 referencing-0.34.0 rpds-py-0.18.0\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy deep-translator python-dotenv python-terrier==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "env = dotenv_values(\".env\")  # replace \".env.example\" with .env file path\n",
    "\n",
    "if (env[\"LANGUAGE_DETECT_API_KEY\"] == \"YOUR API KEY\"):\n",
    "  raise Exception(\"Please replace 'YOUR API KEY' with your actual API key in the .env file\")\n",
    "\n",
    "detection_api_key = env[\"LANGUAGE_DETECT_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import threading\n",
    "\n",
    "import deep_translator as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from deep_translator import GoogleTranslator, single_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "deep-translator version: 1.9.1\n",
      "pyterrier version: 0.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"deep-translator version:\", dt.__version__)\n",
    "print(\"pyterrier version:\", pt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Query formulation\n",
    "\n",
    "Let the end-user determine what they would like to find\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = \"How do I repair my bike?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Query language identification\n",
    "\n",
    "Identify the language of the query with the help of the ... library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    }
   ],
   "source": [
    "l0 = single_detection(q0, api_key=detection_api_key)\n",
    "print(l0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query translation\n",
    "\n",
    "Translate the query into Dutch, English, French, German, Italian, Portuguese, Russian, Spanish, and Chineese. Exlude the original language of the query from the translation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'How do I repair my bike?', 'fr': 'Comment réparer mon vélo ?', 'it': 'Come riparo la mia bicicletta?', 'es': '¿Cómo reparo mi bicicleta?'}\n"
     ]
    }
   ],
   "source": [
    "qs = dict({l0: q0})\n",
    "\n",
    "languages = [\"en\", \"fr\", \"it\", \"es\"]\n",
    "\n",
    "for lang in languages:\n",
    "    if lang != l0:\n",
    "        qs[lang] = translated = GoogleTranslator(source=l0, target=lang).translate(\n",
    "            text=q0\n",
    "        )\n",
    "\n",
    "print(qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search for documents in the target language\n",
    "\n",
    "Search for documents in the target language using the translated queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. MLWIKIR: APython toolkit for building large-scale Wikipedia-based Information Retrieval Datasets in Chinese, English, French, Italian, Japanese, Spanish and more. [Research paper](https://www.irit.fr/CIRCLE/wp-content/uploads/2020/06/CIRCLE20_22.pdf)\n",
    "1. [pyterrier jupyter notebook example of spanish document retreival](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/non_en_retrieval.ipynb)\n",
    "1. [WikIR rawa datasets](https://ir-datasets.com/wikir.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Indexing of the documents\n",
    "If the index of the documents is not available, then the documents will be indexed using the pyterrier library.\n",
    "Otherwise, the index will be loaded from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(dataset: str, index_name: str, fields=[\"text\"]): \n",
    "    \"\"\"\n",
    "    Creates an index for a given dataset using the specified index name and fields.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset object containing the corpus to be indexed.\n",
    "    - index_name: The name of the index to be created.\n",
    "    - fields: A list of fields to be indexed. Default is [\"text\"].\n",
    "\n",
    "    Returns:\n",
    "    - index_ref: The reference to the created index.\n",
    "\n",
    "    \"\"\"\n",
    "    indexer = pt.IterDictIndexer(\"./indices/\" + index_name, verbose=False)\n",
    "    index_ref = indexer.index(dataset.get_corpus_iter(), fields=fields)\n",
    "    return index_ref\n",
    "\n",
    "\n",
    "def find_index(index_name: str):\n",
    "    \"\"\"\n",
    "    Finds the reference to an existing index with the specified name.\n",
    "\n",
    "    Parameters:\n",
    "    - index_name: The name of the index to be found.\n",
    "\n",
    "    Returns:\n",
    "    - index_ref: The reference to the found index.\n",
    "\n",
    "    \"\"\"\n",
    "    return pt.IndexRef.of(\"./indices/\" + index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index wikir_fr14k already exists\n",
      "Index wikir_es13k already exists\n",
      "Index wikir_en1k already exists\n",
      "Index wikir_it16k already exists\n",
      "{'fr': <org.terrier.querying.IndexRef at 0x14785c720 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x136d79900 at 0x14776d030>>, 'es': <org.terrier.querying.IndexRef at 0x1473b1620 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x136d79910 at 0x1523f3630>>, 'en': <org.terrier.querying.IndexRef at 0x151130270 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x136d79930 at 0x1523f3750>>, 'it': <org.terrier.querying.IndexRef at 0x15249d9e0 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x136d79960 at 0x1523f3670>>}\n"
     ]
    }
   ],
   "source": [
    "datasetNames: dict[str, str] = dict({\"fr\":\"wikir/fr14k\", \"es\": \"wikir/es13k\", \"en\":\"wikir/en1k\", \"it\":\"wikir/it16k\"})\n",
    "datasets = dict()\n",
    "indeces = dict()\n",
    "\n",
    "def index(dataset: str, index_name: str, fields=[\"text\"]):\n",
    "    index_ref = create_index(dataset, index_name, fields)\n",
    "    indeces[lang] = index_ref\n",
    "\n",
    "index_threads = []\n",
    "\n",
    "for [lang, datasetName] in datasetNames.items():\n",
    "    datasetFolder = datasetName.replace(\"/\", \"_\")\n",
    "    dataset = pt.get_dataset(\"irds:\"+datasetName)\n",
    "    datasets[lang] = dataset\n",
    "\n",
    "    if os.path.exists(\"./indices/\" + datasetFolder + \"/data.properties\"):\n",
    "        print(\"Index\", datasetFolder, \"already exists\")\n",
    "        index_ref = find_index(datasetFolder)\n",
    "        indeces[lang] = index_ref\n",
    "    else:\n",
    "        print(\"Creating index\", datasetFolder, \" (takes around 1-3 minutes per dataset)\")\n",
    "        thread = threading.Thread(target=index, args=(dataset, datasetFolder))\n",
    "        thread.start()\n",
    "        index_threads.append(thread)\n",
    "\n",
    "for thread in index_threads:\n",
    "    thread.join()\n",
    "\n",
    "print(indeces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Retrieval of the documents\n",
    "The documents will be retrieved using the BM25 retrieval model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fr Index(['qid', 'docid', 'docno', 'rank', 'score', 'query', 'text'], dtype='object') - shape: (5, 7) top 5:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>401968</td>\n",
       "      <td>403301</td>\n",
       "      <td>0</td>\n",
       "      <td>21.629457</td>\n",
       "      <td>Comment rparer mon vlo</td>\n",
       "      <td>elle est issue de la fusion en 1981 de l opéra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>485591</td>\n",
       "      <td>487240</td>\n",
       "      <td>1</td>\n",
       "      <td>21.287983</td>\n",
       "      <td>Comment rparer mon vlo</td>\n",
       "      <td>elle suit la guérison d un possédé muet et fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>195615</td>\n",
       "      <td>196210</td>\n",
       "      <td>2</td>\n",
       "      <td>19.694506</td>\n",
       "      <td>Comment rparer mon vlo</td>\n",
       "      <td>pour saluer bien bas on fait acte de soumissio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>163481</td>\n",
       "      <td>164009</td>\n",
       "      <td>3</td>\n",
       "      <td>19.581240</td>\n",
       "      <td>Comment rparer mon vlo</td>\n",
       "      <td>l histoire raconte comment les personnages des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>129221</td>\n",
       "      <td>129661</td>\n",
       "      <td>4</td>\n",
       "      <td>19.424128</td>\n",
       "      <td>Comment rparer mon vlo</td>\n",
       "      <td>le dæmon serait donc en quelque sorte la manif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid   docno  rank      score                    query  \\\n",
       "0   1  401968  403301     0  21.629457  Comment rparer mon vlo    \n",
       "1   1  485591  487240     1  21.287983  Comment rparer mon vlo    \n",
       "2   1  195615  196210     2  19.694506  Comment rparer mon vlo    \n",
       "3   1  163481  164009     3  19.581240  Comment rparer mon vlo    \n",
       "4   1  129221  129661     4  19.424128  Comment rparer mon vlo    \n",
       "\n",
       "                                                text  \n",
       "0  elle est issue de la fusion en 1981 de l opéra...  \n",
       "1  elle suit la guérison d un possédé muet et fai...  \n",
       "2  pour saluer bien bas on fait acte de soumissio...  \n",
       "3  l histoire raconte comment les personnages des...  \n",
       "4  le dæmon serait donc en quelque sorte la manif...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "document_data: dict[str, pd.DataFrame] = dict()\n",
    "\n",
    "for lang in languages:\n",
    "    index_ref = indeces[lang]\n",
    "    dataset = datasets[lang]\n",
    "\n",
    "    pipeline = pt.BatchRetrieve(\n",
    "        index_ref, wmodel=\"BM25\", metadata=[\"docno\"], num_results=5\n",
    "    ) >> pt.text.get_text(dataset, \"text\")\n",
    "\n",
    "    sanitised_query = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", qs[lang])\n",
    "\n",
    "    pandas_df: pd.DataFrame = pipeline.search(sanitised_query)\n",
    "    document_data[lang] = pandas_df\n",
    "\n",
    "print(\n",
    "    \"Results for fr\",\n",
    "    document_data[\"fr\"].keys(),\n",
    "    \"- shape:\",\n",
    "    document_data[\"fr\"].shape,\n",
    "    \"top 5:\"\n",
    ")\n",
    "\n",
    "document_data[\"fr\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120\n"
     ]
    }
   ],
   "source": [
    "top_document = document_data[\"fr\"].head(1)[\"text\"].values[0]\n",
    "\n",
    "print(len(top_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document translation\n",
    "\n",
    "Translate the documents back to English to be processed by other algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NotImplementedType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m documents_translated: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lang, docs \u001b[38;5;129;01min\u001b[39;00m documents\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     documents_translated[lang] \u001b[38;5;241m=\u001b[39m [GoogleTranslator(source\u001b[38;5;241m=\u001b[39mlang, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(text\u001b[38;5;241m=\u001b[39md) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NotImplementedType' object is not iterable"
     ]
    }
   ],
   "source": [
    "documents_translated: Dict[str, list[str]] = dict()\n",
    "\n",
    "for lang, docs in documents.items():\n",
    "    documents_translated[lang] = [\n",
    "        GoogleTranslator(source=lang, target=\"en\").translate(text=d) for d in docs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find domain-specific keywords\n",
    "\n",
    "Find the most frequent words in the documents, exlude the 1000 most used words in the English language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Concat the keywords with the original query\n",
    "\n",
    "Concatenate the keywords with the original query and search for documents in the original language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Evaluate the results of the search\n",
    "\n",
    "Reference:\n",
    "1. GitHub: [pyterrier/examples/notebooks\n",
    "/retrieval_and_evaluation.ipynb](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/retrieval_and_evaluation.ipynb)\n",
    "1. GitHub: [pyterrier/examples/notebooks\n",
    "/experiment.ipynb](https://github.com/terrier-org/pyterrier/blob/master/examples/notebooks/experiment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Precision\n",
    "\n",
    "See how many of the returned documents are relevant. Did the number of relevant documents increase?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Keyword diversification\n",
    "\n",
    "Did the number of unique keywords increase compared to naive domain-specific keyword identification?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
